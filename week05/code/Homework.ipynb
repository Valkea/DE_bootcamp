{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81abe327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ec0ce",
   "metadata": {},
   "source": [
    "### Let's start a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185bd2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 19:11:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41dbe85",
   "metadata": {},
   "source": [
    "## Question 1. Install Spark and PySpark\n",
    "\n",
    "> - Install Spark\n",
    "> - Run PySpark\n",
    "> - Create a local spark session\n",
    "> - Execute spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c3e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/25 19:11:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/25 19:11:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Spark context Web UI available at http://de-zoomcamp.europe-west1-b.c.lexical-passkey-375922.internal:4041\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1677352313068).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.2)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\u001b[35m\n",
      "scala> \u001b[0m\n",
      "\u001b[35m\n",
      "scala> \u001b[0m"
     ]
    }
   ],
   "source": [
    "!spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf766bcc",
   "metadata": {},
   "source": [
    "> Q1 ANSWER: **res0: String = 3.3.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e2bf1",
   "metadata": {},
   "source": [
    "## Question 2: HVFHW June 2021\n",
    "\n",
    "> - Read it with Spark using the same schema as we did in the lessons.\n",
    "> - We will use this dataset for all the remaining questions.\n",
    "> - Repartition it to 12 partitions and save it to parquet.\n",
    "\n",
    "> **What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c164fd4",
   "metadata": {},
   "source": [
    "### Let's convert raw data to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82eb142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.IntegerType(), True),\n",
    "    types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"dropOff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField(\"SR_Flag\", types.StringType(), True),    \n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca96e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== COLOR: fhvhv =====\n",
      "----- YEAR: 2021 -----\n",
      "----- MONTH: 6 -----\n",
      "processing fhvhv data for 2021/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "range_color = ['fhvhv']\n",
    "range_year = [2021]\n",
    "range_month = [6]\n",
    "\n",
    "for color in range_color:\n",
    "    print(f\"===== COLOR: {color} =====\")\n",
    "    \n",
    "    for year in range_year:\n",
    "        print(f\"----- YEAR: {year} -----\")\n",
    "        \n",
    "        for month in range_month:\n",
    "            print(f\"----- MONTH: {month} -----\")\n",
    "            \n",
    "            try:\n",
    "                print(f'processing {color} data for {year}/{month}')\n",
    "\n",
    "                input_path = f'data/raw/{color}/{year}/{month:02d}/'\n",
    "                output_path = f'data/pq/{color}/{year}/{month:02d}/'\n",
    "\n",
    "                # df_tmp = spark.read \\\n",
    "                #     .option(\"header\", \"true\") \\\n",
    "                #     .option(\"inferSchema\" , \"true\") \\\n",
    "                #     .csv(input_path)\n",
    "                \n",
    "                df_tmp = spark.read \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .schema(eval(f\"{color}_schema\")) \\\n",
    "                    .csv(input_path)\n",
    "\n",
    "                df_tmp \\\n",
    "                    .repartition(12) \\\n",
    "                    .write.parquet(output_path, mode='overwrite')\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb13e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 275M\r\n",
      "-rw-r--r-- 1 valkea valkea   0 Feb 25 19:15 _SUCCESS\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00000-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00001-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00002-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00003-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00004-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00005-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00006-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00007-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00008-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00009-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00010-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 valkea valkea 23M Feb 25 19:15 part-00011-2cc3d426-cee8-4166-a42a-a13523754e31-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/pq/fhvhv/2021/06/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0209e113",
   "metadata": {},
   "source": [
    "> Q2 ANSWER: **23M** ==> **24MB**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbea129",
   "metadata": {},
   "source": [
    "## Question 3: Count records\n",
    "\n",
    "> How many taxi trips were there on June 15? (Consider only trips that started on June 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abcac4",
   "metadata": {},
   "source": [
    "### Let's load the parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39afe15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv = spark.read.parquet('data/pq/fhvhv/2021/06') # data/pq/green/all_years/all_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15177540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6a78e",
   "metadata": {},
   "source": [
    "### Let's make SQL requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e85986",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv.createOrReplaceTempView('fhvhv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db446da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|number_records|\n",
      "+--------------+\n",
      "|      14961892|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv_result = spark.sql(\"\"\"\n",
    "    SELECT     \n",
    "        COUNT(1) AS number_records\n",
    "    FROM fhvhv_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2b409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0567fda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|number_records|\n",
      "+--------------+\n",
      "|        452470|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhvhv_result = spark.sql(\"\"\"\n",
    "    SELECT     \n",
    "        COUNT(1) AS number_records\n",
    "    FROM fhvhv_data\n",
    "    WHERE pickup_datetime >= '2021-06-15 00:00:00' AND pickup_datetime < '2021-06-16 00:00:00'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16aace0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|                day|number_records|\n",
      "+-------------------+--------------+\n",
      "|2021-06-01 00:00:00|        417375|\n",
      "|2021-06-02 00:00:00|        457339|\n",
      "|2021-06-03 00:00:00|        521408|\n",
      "|2021-06-04 00:00:00|        538917|\n",
      "|2021-06-05 00:00:00|        604903|\n",
      "|2021-06-06 00:00:00|        522753|\n",
      "|2021-06-07 00:00:00|        425771|\n",
      "|2021-06-08 00:00:00|        462554|\n",
      "|2021-06-09 00:00:00|        483353|\n",
      "|2021-06-10 00:00:00|        504108|\n",
      "|2021-06-11 00:00:00|        549286|\n",
      "|2021-06-12 00:00:00|        591339|\n",
      "|2021-06-13 00:00:00|        509039|\n",
      "|2021-06-14 00:00:00|        426672|\n",
      "|2021-06-15 00:00:00|        452470|\n",
      "|2021-06-16 00:00:00|        479776|\n",
      "|2021-06-17 00:00:00|        497133|\n",
      "|2021-06-18 00:00:00|        540056|\n",
      "|2021-06-19 00:00:00|        601189|\n",
      "|2021-06-20 00:00:00|        491630|\n",
      "+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhvhv_result = spark.sql(\"\"\"\n",
    "    SELECT     \n",
    "        date_trunc('day', pickup_datetime) AS day,\n",
    "        COUNT(1) AS number_records\n",
    "    FROM fhvhv_data\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ff663",
   "metadata": {},
   "source": [
    "> Q3 ANSWER: **452470** ==> **452,470**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8feba5",
   "metadata": {},
   "source": [
    "## Question 4: Longest trip for each day\n",
    "\n",
    "> How long was the longest trip in Hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "995f20a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|duration_hours|number_records|\n",
      "+--------------+--------------+\n",
      "|            66|             1|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhvhv_result = spark.sql(\"\"\"\n",
    "    SELECT     \n",
    "        DATEDIFF(hour, pickup_datetime, dropOff_datetime) as duration_hours,\n",
    "        COUNT(1) AS number_records\n",
    "    FROM fhvhv_data\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1 DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b6e0b",
   "metadata": {},
   "source": [
    "> Q4 ANSWER: **66** ==> **66.87**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724afa0",
   "metadata": {},
   "source": [
    "## Question 5: User Interface\n",
    "\n",
    "> Spark’s User Interface which shows application's dashboard runs on which local port?\n",
    "\n",
    "> Q5 ANSWER: **4040**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd37a7",
   "metadata": {},
   "source": [
    "## Question 6: Most frequent pickup location zone\n",
    "\n",
    "> Load the zone lookup data into a temp view in Spark Zone Data\n",
    "\n",
    "> Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9092c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('data/taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffd42017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85eadf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f34dd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_fhvhv.join(df_zone, df_fhvhv.PULocationID == df_zone.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bd8008d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b79cdd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.createOrReplaceTempView('join_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e1fd651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|               Zone|number_records|\n",
      "+-------------------+--------------+\n",
      "|Crown Heights North|        231279|\n",
      "+-------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join_result = spark.sql(\"\"\"\n",
    "    SELECT     \n",
    "        Zone,\n",
    "        COUNT(1) AS number_records\n",
    "    FROM join_data\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e496f4f",
   "metadata": {},
   "source": [
    "> Q6 ANSWER: **Crown Heights North**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
